{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN4SubXdOyGh",
        "outputId": "bf1b03ac-8d26-4471-fac3-3cfdb7e4b843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.13\n",
            "CUDA available: True\n",
            "Mon Aug  4 04:12:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             43W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Torch: 2.5.1+cu121 | Has nn.RMSNorm: True\n"
          ]
        }
      ],
      "source": [
        "import sys, torch\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "!nvidia-smi || echo \"No GPU detected â€” set Runtime â†’ Change runtime type â†’ GPU\"\n",
        "try:\n",
        "    print(\"Torch:\", torch.__version__, \"| Has nn.RMSNorm:\", hasattr(torch.nn, \"RMSNorm\"))\n",
        "except Exception as e:\n",
        "    print(\"Torch import issue:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj8gIr3iOyut",
        "outputId": "9573b643-88bb-4930-b635-40352e9bff3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Existing Torch: 2.5.1+cu121 | Has nn.RMSNorm: True\n",
            "> pip install -U --quiet transformers peft datasets accelerate safetensors sentencepiece\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, sys\n",
        "\n",
        "def run(cmd):\n",
        "    print(\">\", cmd)\n",
        "    return subprocess.run(cmd.split(), check=False, text=True, capture_output=True)\n",
        "\n",
        "need_restart = False\n",
        "\n",
        "# 1) Ensure a recent CUDA 12.1 PyTorch with nn.RMSNorm available\n",
        "try:\n",
        "    import torch\n",
        "    has_rms = hasattr(torch.nn, \"RMSNorm\")\n",
        "    print(\"Existing Torch:\", torch.__version__, \"| Has nn.RMSNorm:\", has_rms)\n",
        "except Exception:\n",
        "    has_rms = False\n",
        "    print(\"No working torch yet.\")\n",
        "\n",
        "if not has_rms:\n",
        "    # Install CUDA 12.1 wheels\n",
        "    print(\"Installing/upgrading PyTorch (CUDA 12.1)â€¦\")\n",
        "    print(run(\"pip install -U --quiet --index-url https://download.pytorch.org/whl/cu121 torch torchvision\").stdout)\n",
        "    need_restart = True\n",
        "\n",
        "# 2) Core stack (safe to (re)install without restart)\n",
        "print(run(\"pip install -U --quiet transformers peft datasets accelerate safetensors sentencepiece\").stdout)\n",
        "\n",
        "# 3) Restart ONLY if PyTorch was (re)installed so new binaries load\n",
        "if need_restart:\n",
        "    print(\"\\nðŸ” Restarting runtime to load new PyTorch (this is expected). After it reconnects, re-run Cell 1, then continue.\")\n",
        "    import IPython\n",
        "    IPython.get_ipython().kernel.do_shutdown(True)  # clean restart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxEEL58AO0uT",
        "outputId": "7c96f789-0dec-42b0-f411-de13083d44ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.13\n",
            "Torch: 2.5.1+cu121 | Has nn.RMSNorm: True\n",
            "Transformers: 4.54.1\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers, sys\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Torch:\", torch.__version__, \"| Has nn.RMSNorm:\", hasattr(torch.nn, \"RMSNorm\"))\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
        "assert hasattr(torch.nn, \"RMSNorm\"), \"Torch is too old; go back to Cell 2.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AxEsiK-O2wv",
        "outputId": "ba0d17f3-c471-4fa1-d360-8904a966f1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF token set? True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Only needed if your BASE model is gated (e.g., meta-llama/*). Otherwise skip.\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"hf_xxx_your_token_here\"\n",
        "print(\"HF token set?\", bool(os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKeSxwdYPZBQ",
        "outputId": "81f6d7b8-46ab-4056-fe3b-ad14aeaf3e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/train_lora.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/train_lora.py\n",
        "#!/usr/bin/env python3\n",
        "import os, argparse, math, hashlib, random\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    return \" \".join(s.lower().strip().split())\n",
        "\n",
        "def smart_subsample(jsonl_path, tokenizer, target_minutes, step_time_guess, bsz, grad_accum, epochs, seed=0):\n",
        "    \"\"\"\n",
        "    Returns a tokenized Dataset limited to a size that should train in ~target_minutes.\n",
        "    Strategy:\n",
        "      1) Build a clean 'text' column from JSONL chat messages.\n",
        "      2) Deduplicate by normalized user+assistant text hash (keeps first).\n",
        "      3) Compute token lengths; bin into quartiles; sample evenly from bins\n",
        "         (preserves short/medium/long distribution).\n",
        "      4) Tokenize only the selected subset; drop 'text' column.\n",
        "    \"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    # Load raw jsonl\n",
        "    raw = load_dataset(\"json\", data_files=str(jsonl_path), split=\"train\")\n",
        "\n",
        "    # Build a plain 'text' column using the tokenizer's chat template if available\n",
        "    def _to_text(example):\n",
        "        msgs = example[\"messages\"]\n",
        "        if hasattr(tokenizer, \"apply_chat_template\"):\n",
        "            txt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
        "        else:\n",
        "            user = next((m[\"content\"] for m in msgs if m[\"role\"] == \"user\"), \"\")\n",
        "            assistant = next((m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"), \"\")\n",
        "            txt = f\"<s>[INST]{user}[/INST]\\n{assistant}</s>\"\n",
        "        return {\"text\": txt}\n",
        "    raw = raw.map(_to_text, remove_columns=raw.column_names)\n",
        "\n",
        "    # Deduplicate near-duplicates\n",
        "    seen = set()\n",
        "    def _dedup(example):\n",
        "        key = hashlib.sha1(normalize_text(example[\"text\"]).encode(\"utf-8\")).hexdigest()\n",
        "        if key in seen:\n",
        "            return {\"keep\": 0}\n",
        "        seen.add(key)\n",
        "        return {\"keep\": 1}\n",
        "    raw = raw.map(_dedup)\n",
        "    raw = raw.filter(lambda e: e[\"keep\"] == 1).remove_columns([\"keep\"])\n",
        "\n",
        "    # Fast length pass (no padding, truncation to cap work)\n",
        "    def _len_only(example):\n",
        "        ids = tokenizer(example[\"text\"], add_special_tokens=False, truncation=True, max_length=1024)[\"input_ids\"]\n",
        "        return {\"_len\": len(ids)}\n",
        "    raw = raw.map(_len_only)\n",
        "\n",
        "    n_total = len(raw)\n",
        "    # Compute effective batch and steps per epoch we can afford for target_minutes\n",
        "    eff_batch = bsz * grad_accum\n",
        "    steps_total_allowed = int(target_minutes * 60.0 / max(step_time_guess, 1e-6))\n",
        "    steps_per_epoch = max(1, steps_total_allowed // max(epochs, 1))\n",
        "    n_target = min(n_total, max(eff_batch, steps_per_epoch * eff_batch))\n",
        "\n",
        "    # Bin into quartiles by length\n",
        "    # Sort by _len and split into 4 roughly equal bins\n",
        "    raw_sorted = raw.sort(\"_len\")\n",
        "    qsize = max(1, len(raw_sorted) // 4)\n",
        "    bins = [\n",
        "        raw_sorted.select(range(0, qsize)),\n",
        "        raw_sorted.select(range(qsize, 2*qsize)),\n",
        "        raw_sorted.select(range(2*qsize, 3*qsize)),\n",
        "        raw_sorted.select(range(3*qsize, len(raw_sorted))),\n",
        "    ]\n",
        "    # Sample evenly from bins (as even as possible)\n",
        "    per_bin = [n_target // 4] * 4\n",
        "    rem = n_target - sum(per_bin)\n",
        "    for i in range(rem): per_bin[i % 4] += 1\n",
        "\n",
        "    # Random indices from each bin\n",
        "    selected = []\n",
        "    for b, k in zip(bins, per_bin):\n",
        "        idxs = list(range(len(b)))\n",
        "        rng.shuffle(idxs)\n",
        "        idxs = idxs[:k]\n",
        "        # Map local bin indices back to global raw_sorted indices\n",
        "        base = b._indices if hasattr(b, \"_indices\") else None\n",
        "        # b is a SelectDataset; compute global indices:\n",
        "        # We'll use the original positions from raw_sorted via .select\n",
        "        selected.extend([b[i] for i in idxs])  # materialize examples (ok for ~10k)\n",
        "\n",
        "    # Create a new Dataset from selected examples\n",
        "    from datasets import Dataset\n",
        "    sampled = Dataset.from_list(selected)\n",
        "\n",
        "    # Final tokenize with padding to multiple of 16, drop 'text'\n",
        "    tokenized = sampled.map(\n",
        "        lambda ex: tokenizer(ex[\"text\"], truncation=True, max_length=1024),\n",
        "        batched=True,\n",
        "        remove_columns=[\"text\"],\n",
        "    )\n",
        "    # Report planning numbers\n",
        "    est_steps = math.ceil(len(tokenized) / eff_batch) * max(epochs, 1)\n",
        "    est_minutes = est_steps * step_time_guess / 60.0\n",
        "    print(f\"[Sampler] total_raw={n_total}  kept_dedup={len(raw)}  target={len(tokenized)}\")\n",
        "    print(f\"[Sampler] eff_batch={eff_batch}  epochs={epochs}  est_steps={est_steps}  est_timeâ‰ˆ{est_minutes:.1f} min\")\n",
        "    return tokenized\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--base\", default=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "    ap.add_argument(\"--data\", required=True)\n",
        "    ap.add_argument(\"--out\", required=True)\n",
        "    ap.add_argument(\"--epochs\", type=int, default=1)          # 1 epoch for a 10-min pass\n",
        "    ap.add_argument(\"--lr\", type=float, default=2e-4)\n",
        "    ap.add_argument(\"--bsz\", type=int, default=4)             # good on A100\n",
        "    ap.add_argument(\"--grad_accum\", type=int, default=4)      # eff batch 16\n",
        "    ap.add_argument(\"--target_minutes\", type=float, default=10.0)\n",
        "    ap.add_argument(\"--step_time_guess\", type=float, default=1.1)  # s/step on A100 with LoRA\n",
        "    ap.add_argument(\"--seed\", type=int, default=0)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA not available. Set Runtime â†’ Change runtime type â†’ GPU.\")\n",
        "\n",
        "    # A100 fast-path\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    major_cc = torch.cuda.get_device_capability()[0]\n",
        "    use_bf16 = major_cc >= 8\n",
        "    print(f\"[Device] {device_name}  bf16={use_bf16}\")\n",
        "\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(args.base, use_fast=False, token=os.environ.get(\"HUGGINGFACE_HUB_TOKEN\"))\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "    # Smart subsample BEFORE tokenizing everything\n",
        "    ds = smart_subsample(\n",
        "        jsonl_path=args.data,\n",
        "        tokenizer=tok,\n",
        "        target_minutes=args.target_minutes,\n",
        "        step_time_guess=args.step_time_guess,\n",
        "        bsz=args.bsz,\n",
        "        grad_accum=args.grad_accum,\n",
        "        epochs=args.epochs,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "\n",
        "    # Build model on single GPU (no sharding), SDPA attention\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        args.base,\n",
        "        token=os.environ.get(\"HUGGINGFACE_HUB_TOKEN\"),\n",
        "        torch_dtype=(torch.bfloat16 if use_bf16 else torch.float16),\n",
        "        low_cpu_mem_usage=True,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        device_map=None,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # LoRA config\n",
        "    lcfg = LoraConfig(\n",
        "        r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lcfg)\n",
        "\n",
        "    # No gradient checkpointing on A100 (keeps it fast)\n",
        "    # model.gradient_checkpointing_enable()  # <- leave OFF on A100\n",
        "    # model.enable_input_require_grads()     # <- not needed without checkpointing\n",
        "    # model.config.use_cache = False         # <- default ok\n",
        "\n",
        "    collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False, pad_to_multiple_of=16)\n",
        "\n",
        "    targs = TrainingArguments(\n",
        "        output_dir=args.out,\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.bsz,\n",
        "        gradient_accumulation_steps=args.grad_accum,\n",
        "        learning_rate=args.lr,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        weight_decay=0.0,\n",
        "        logging_steps=25,\n",
        "        save_strategy=\"no\",   # skip epoch saves to save time; you can switch to 'epoch'\n",
        "        report_to=[],\n",
        "        bf16=use_bf16,\n",
        "        fp16=not use_bf16,\n",
        "        optim=\"adamw_torch\",\n",
        "        group_by_length=True,\n",
        "        dataloader_num_workers=8,\n",
        "        dataloader_pin_memory=True,\n",
        "        gradient_checkpointing=False,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(model=model, args=targs, train_dataset=ds, data_collator=collator)\n",
        "    trainer.train()\n",
        "    model.save_pretrained(args.out)\n",
        "    print(\"Saved adapter to\", args.out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "WQtxtT8PPbRh",
        "outputId": "27e03c3e-02b9-4f52-98f2-99cb28a28f6a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6a740b9306c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pick training_data_XXXX.jsonl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mJSONL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mJSONL_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "up = files.upload()  # pick training_data_XXXX.jsonl\n",
        "JSONL_PATH = list(up.keys())[0]\n",
        "JSONL_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBxnHluaPjq3",
        "outputId": "f2c246c1-adbc-4fc7-8988-af0944398bf6"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "OUT_DIR   = \"/content/adapters/texttwin-XXXXXXXXXX\"\n",
        "EPOCHS    = 1                  # 1 epoch for a ~10-min pass\n",
        "LR        = 2e-4\n",
        "BSZ       = 4                  # try 4; if VRAM is huge you can try 8\n",
        "GRAD_ACC  = 4                  # eff batch = 16\n",
        "TARGET_MIN= 10\n",
        "STEP_GUESS= 1.1                # seconds/step (adjust to your observed speed)\n",
        "\n",
        "print(\"Base:\", BASE_MODEL)\n",
        "print(\"Data:\", JSONL_PATH)\n",
        "print(\"Out:\", OUT_DIR)\n",
        "\n",
        "!python3 /content/train_lora.py \\\n",
        "  --base \"$BASE_MODEL\" \\\n",
        "  --data \"$JSONL_PATH\" \\\n",
        "  --out  \"$OUT_DIR\" \\\n",
        "  --epochs $EPOCHS --lr $LR --bsz $BSZ --grad_accum $GRAD_ACC \\\n",
        "  --target_minutes $TARGET_MIN --step_time_guess $STEP_GUESS --seed 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Oxc9ku0GPogX",
        "outputId": "771ab1c3-0cb4-46e8-e9cf-bad562b448bd"
      },
      "outputs": [],
      "source": [
        "import os, zipfile\n",
        "zip_path = OUT_DIR + \".zip\"\n",
        "def zip_dir(src_dir, zip_path):\n",
        "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "        for root, _, files in os.walk(src_dir):\n",
        "            for f in files:\n",
        "                full = os.path.join(root, f)\n",
        "                rel = os.path.relpath(full, os.path.dirname(src_dir))\n",
        "                zf.write(full, rel)\n",
        "zip_dir(OUT_DIR, zip_path)\n",
        "!ls -lh \"$zip_path\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ATQcjGgbPqxW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "cs224n"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
